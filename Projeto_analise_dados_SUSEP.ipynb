{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b62467eb-64f6-47c8-ad57-73f216fa0295",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Criação de Views Temporárias a partir de Arquivos CSV por Ano e Semestre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "3493ea39-941b-4bc4-9c33-8f27e39c49c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importar bibliotecas necessárias\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Função simples para carregar um arquivo CSV e criar uma view temporária\n",
    "def carregar_csv(nome_tabela, caminho_csv):\n",
    "    # Ler o arquivo CSV\n",
    "    df = spark.read.format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .option(\"sep\", ';') \\\n",
    "        .load(caminho_csv)\n",
    "    \n",
    "    # Criar uma view temporária\n",
    "    df.createOrReplaceTempView(nome_tabela)\n",
    "    \n",
    "    # Exibir as primeiras 10 linhas da tabela temporária para conferir\n",
    "    print(f\"\\nPrimeiras 10 linhas da tabela {nome_tabela}:\")\n",
    "    spark.sql(f\"SELECT * FROM {nome_tabela} LIMIT 10\").show()\n",
    "\n",
    "# Lista de anos e semestres - Arquivos são nomeados como: auto_cau_5A\n",
    "anos_semestres = [\n",
    "    (\"2015\", \"A\"), (\"2015\", \"B\"),\n",
    "    (\"2016\", \"A\"), (\"2016\", \"B\"),\n",
    "    (\"2017\", \"A\"), (\"2017\", \"B\"),\n",
    "    (\"2018\", \"A\"), (\"2018\", \"B\"),\n",
    "    (\"2019\", \"A\"), (\"2019\", \"B\")\n",
    "]\n",
    "\n",
    "# Lista dos arquivos que preciso carregar\n",
    "nomes_base = [\n",
    "    \"auto_cau\", \"auto_cat\", \"auto_cep\", \"auto_cidade\", \"auto_cob\",\n",
    "    \"auto_idade\", \"auto_reg\", \"auto_sexo\", \"auto2_grupo\", \"auto2_vei\",\n",
    "    \"PremReg\", \"arq_casco_comp\", \"arq_casco3_comp\", \"arq_casco4_comp\", \"SinReg\"\n",
    "]\n",
    "\n",
    "# Loop simples para carregar os arquivos e criar views temporárias\n",
    "for ano, semestre in anos_semestres:\n",
    "    for nome_base in nomes_base:\n",
    "        # Criar o nome do arquivo no formato auto_cau_5A, auto_cat_5A, etc.\n",
    "        nome_arquivo = f\"{nome_base}_{ano[-1]}{semestre}\"\n",
    "        caminho_csv = f\"dbfs:/FileStore/tables/{nome_arquivo}.csv\"\n",
    "        \n",
    "        # Chamar a função para carregar o CSV e criar a view\n",
    "        carregar_csv(nome_arquivo, caminho_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bed4d103-d63b-4c75-b78a-c7d3bf80141e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Concatenar os semestres por ano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a4aa38e2-5ae2-47c8-9c3d-8069afa20d9b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importar função necessária para adicionar colunas\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Função para concatenar arquivos de um tipo específico por ano e semestre\n",
    "def concatenar_arquivos_por_tipo(nome_base):\n",
    "    \"\"\"\n",
    "    Concatena arquivos CSV de vários anos e semestres em uma única tabela temporária.\n",
    "    \n",
    "    :param nome_base: O nome base dos arquivos CSV a serem lidos.\n",
    "    \"\"\"\n",
    "    # Lista para armazenar DataFrames dos arquivos\n",
    "    dfs = []\n",
    "    \n",
    "    # Iterar sobre os anos e semestres\n",
    "    for ano in [\"2015\", \"2016\", \"2017\", \"2018\", \"2019\"]:\n",
    "        for semestre in [\"A\", \"B\"]:\n",
    "            # Gerar o nome do arquivo com base no ano e semestre\n",
    "            nome_arquivo = f\"{nome_base}_{ano[-1]}{semestre}\"\n",
    "            caminho_csv = f\"dbfs:/FileStore/tables/{nome_arquivo}.csv\"\n",
    "            \n",
    "            # Carregar o arquivo CSV no Spark DataFrame\n",
    "            df = spark.read.format(\"csv\") \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .option(\"inferSchema\", \"true\") \\\n",
    "                .option(\"sep\", ';') \\\n",
    "                .load(caminho_csv)\n",
    "            \n",
    "            # Adicionar uma coluna para o ano que o dado foi registrado\n",
    "            df = df.withColumn(\"ano_registro\", lit(ano))\n",
    "            \n",
    "            # Adicionar o DataFrame à lista\n",
    "            dfs.append(df)\n",
    "    \n",
    "    # Verificar se os esquemas dos DataFrames são compatíveis\n",
    "    all_columns = set(df.columns for df in dfs)\n",
    "    \n",
    "    # Ajustar os DataFrames para garantir o mesmo esquema\n",
    "    for i in range(len(dfs)):\n",
    "        df = dfs[i]\n",
    "        missing_columns = all_columns - set(df.columns)\n",
    "        for column in missing_columns:\n",
    "            df = df.withColumn(column, lit(None))\n",
    "        dfs[i] = df.select(sorted(all_columns))  # Ordenar colunas para padronizar\n",
    "    \n",
    "    # Concatenar todos os DataFrames em um único\n",
    "    df_concatenado = dfs[0]\n",
    "    for df in dfs[1:]:\n",
    "        df_concatenado = df_concatenado.union(df)\n",
    "    \n",
    "    # Criar uma view temporária com o DataFrame concatenado\n",
    "    nome_tabela_final = f\"dados_{nome_base}\"\n",
    "    df_concatenado.createOrReplaceTempView(nome_tabela_final)\n",
    "    \n",
    "    # Mostrar as primeiras 10 linhas da tabela final\n",
    "    print(f\"\\nTabela: {nome_tabela_final}\")\n",
    "    spark.sql(f\"SELECT * FROM {nome_tabela_final} LIMIT 10\").display()\n",
    "\n",
    "# Lista de tipos de arquivos para processamento\n",
    "tipos_arquivos = [\n",
    "    \"SinReg\", \"auto_cau\", \"auto_cat\", \"auto_cep\", \"auto_cidade\", \"auto_cob\",\n",
    "    \"auto_idade\", \"auto_reg\", \"auto_sexo\", \"auto2_grupo\", \"auto2_vei\",\n",
    "    \"PremReg\", \"arq_casco_comp\", \"arq_casco3_comp\", \"arq_casco4_comp\"\n",
    "]\n",
    "\n",
    "# Processar e concatenar todos os arquivos por tipo\n",
    "for tipo in tipos_arquivos:\n",
    "    concatenar_arquivos_por_tipo(tipo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "fb90cadf-afe0-4594-bc61-2f1b6d757b53",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lista de tipos de arquivos para visualizar\n",
    "tipos_arquivos = [\n",
    "    \"SinReg\", \"auto_cau\", \"auto_cat\", \"auto_cep\", \"auto_cidade\", \"auto_cob\",\n",
    "    \"auto_idade\", \"auto_reg\", \"auto_sexo\", \"auto2_grupo\", \"auto2_vei\",\n",
    "    \"PremReg\", \"arq_casco_comp\", \"arq_casco3_comp\", \"arq_casco4_comp\"\n",
    "]\n",
    "\n",
    "# Exibe as primeiras 50 linhas de cada tabela temporária criada\n",
    "for tipo in tipos_arquivos:\n",
    "    nome_tabela_final = f\"dados_{tipo}\"  # Nome da tabela temporária\n",
    "    print(f\"\\nExibindo a tabela: {nome_tabela_final}\")  # Mostra qual tabela está sendo exibida\n",
    "    consulta = f\"SELECT * FROM {nome_tabela_final} LIMIT 50\"  \n",
    "    resultado = spark.sql(consulta)  \n",
    "    resultado.show()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6eafdf89-8e72-4c9e-9a5e-b499152c440b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Concatenar e Salvar os DataFrames\n",
    "- Salvar os DataFrames me ajuda a economizar tempo na execução do código, especialmente quando o cluster expira."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "84f45e73-5968-4231-b851-d626e9d79219",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importando a função necessária\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Função para juntar e salvar arquivos no formato Parquet\n",
    "def concatenar_e_salvar_arquivos_por_tipo(nome_base):\n",
    "    # Lista para guardar os DataFrames\n",
    "    dfs = []\n",
    "    \n",
    "    # Fazer a leitura dos arquivos de cada ano e semestre\n",
    "    for ano in [\"2015\", \"2016\", \"2017\", \"2018\", \"2019\"]:\n",
    "        for semestre in [\"A\", \"B\"]:\n",
    "            # Cria o nome do arquivo com base no ano e semestre\n",
    "            nome_arquivo = f\"{nome_base}_{ano[-1]}{semestre}\"\n",
    "            \n",
    "            # Lê o arquivo CSV e cria um DataFrame\n",
    "            caminho_csv = f\"dbfs:/FileStore/tables/{nome_arquivo}.csv\"\n",
    "            df = spark.read.format(\"csv\") \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .option(\"inferSchema\", \"true\") \\\n",
    "                .option(\"sep\", ';') \\\n",
    "                .load(caminho_csv)\n",
    "\n",
    "            # Adicionar uma coluna para o ano que o dado foi registrado\n",
    "            df = df.withColumn(\"ano_registro\", lit(ano))\n",
    "\n",
    "            # Adiciona o DataFrame à lista\n",
    "            dfs.append(df)\n",
    "    \n",
    "    # Junta todos os DataFrames em um só\n",
    "    df_concatenado = dfs[0]\n",
    "    for df in dfs[1:]:\n",
    "        df_concatenado = df_concatenado.union(df)\n",
    "    \n",
    "    # Salva o DataFrame concatenado no formato Parquet\n",
    "    caminho_dbfs = f\"dbfs:/FileStore/tables/{nome_base}_concatenado.parquet\"\n",
    "    df_concatenado.write.mode(\"overwrite\").parquet(caminho_dbfs)\n",
    "    print(f\"DataFrame {nome_base} salvo com sucesso no caminho {caminho_dbfs}\")\n",
    "\n",
    "# Lista dos tipos de arquivos que serão processados e salvos\n",
    "tipos_arquivos = [\n",
    "    \"SinReg\", \"auto_cau\", \"auto_cat\", \"auto_cep\", \"auto_cidade\", \"auto_cob\",\n",
    "    \"auto_idade\", \"auto_reg\", \"auto_sexo\", \"auto2_grupo\", \"auto2_vei\",\n",
    "    \"PremReg\", \"arq_casco_comp\", \"arq_casco3_comp\", \"arq_casco4_comp\"\n",
    "]\n",
    "\n",
    "# Executar a função para cada tipo de arquivo\n",
    "for tipo in tipos_arquivos:\n",
    "    concatenar_e_salvar_arquivos_por_tipo(tipo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e84b773-eaf0-4509-badd-2dd338149efb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Carregar os DataFrames salvos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "84f209bd-4699-4b1b-a7f0-f406aecf90c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Função para carregar um DataFrame de um arquivo Parquet e criar uma view temporária\n",
    "def carregar_view(nome_base):\n",
    "    # Definir o caminho do arquivo Parquet\n",
    "    caminho_parquet = f\"dbfs:/FileStore/tables/{nome_base}_concatenado.parquet\"\n",
    "    \n",
    "    # Carregar o DataFrame do arquivo\n",
    "    df = spark.read.format(\"parquet\").load(caminho_parquet)\n",
    "    \n",
    "    # Nome da view temporária\n",
    "    nome_view = f\"dados_{nome_base}\"\n",
    "    df.createOrReplaceTempView(nome_view)\n",
    "    \n",
    "    # Mostrar as primeiras 10 linhas da view\n",
    "    print(f\"\\nTabela: {nome_view}\")\n",
    "    spark.sql(f\"SELECT * FROM {nome_view} LIMIT 10\").display()\n",
    "\n",
    "# Lista de tipos de arquivos para carregar\n",
    "tipos_de_arquivos = [\n",
    "    \"SinReg\", \"auto_cau\", \"auto_cat\", \"auto_cep\", \"auto_cidade\", \"auto_cob\",\n",
    "    \"auto_idade\", \"auto_reg\", \"auto_sexo\", \"auto2_grupo\", \"auto2_vei\",\n",
    "    \"PremReg\", \"arq_casco_comp\", \"arq_casco3_comp\", \"arq_casco4_comp\"\n",
    "]\n",
    "\n",
    "# Carregar a view para cada tipo de arquivo\n",
    "for tipo in tipos_de_arquivos:\n",
    "    carregar_view(tipo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d11338c2-ead8-419f-a821-34816dd18fa2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Preparação e Tratamento dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1c74533f-2cba-4e6c-b3f9-9f988fbd5d18",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Função para limpar os dados do DataFrame\n",
    "def limpar_dados(df):\n",
    "    \"\"\"\n",
    "    Essa função vai ajudar a limpar o DataFrame, removendo duplicatas\n",
    "    e registros que estão incompletos.\n",
    "    \"\"\"\n",
    "    # Remover duplicatas\n",
    "    df = df.dropDuplicates()\n",
    "\n",
    "    # Tirar os registros que têm todos os valores nulos\n",
    "    df = df.dropna(how='all')\n",
    "\n",
    "    # Remover registros que tenham algum valor nulo\n",
    "    df = df.dropna()\n",
    "\n",
    "    return df\n",
    "\n",
    "# Limpar cada DataFrame da lista\n",
    "for tipo in tipos_arquivos:\n",
    "    # Definindo o nome da tabela a partir do tipo\n",
    "    nome_tabela_final = f\"dados_{tipo}\"\n",
    "    \n",
    "    # Carregar o DataFrame a partir da view temporária\n",
    "    df = spark.sql(f\"SELECT * FROM {nome_tabela_final}\")\n",
    "    \n",
    "    # Aplicar a limpeza nos dados\n",
    "    df_limpo = limpar_dados(df)\n",
    "    \n",
    "    # Criar uma nova view temporária para os dados limpos\n",
    "    df_limpo.createOrReplaceTempView(nome_tabela_final + \"_limpo\")\n",
    "\n",
    "# Mostrar as primeiras 50 linhas de cada DataFrame limpo\n",
    "for tipo in tipos_arquivos:\n",
    "    nome_tabela_final = f\"dados_{tipo}_limpo\"\n",
    "    print(f\"\\nTabela: {nome_tabela_final}\")\n",
    "    \n",
    "    # Realizando a consulta para mostrar os dados\n",
    "    consulta = f\"SELECT * FROM {nome_tabela_final} LIMIT 50\"\n",
    "    resultado = spark.sql(consulta)\n",
    "    \n",
    "    # Exibindo os resultados\n",
    "    resultado.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7a23722-06a1-4e97-9787-12485e80ddf5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Tratamento de Valores Ausentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "38e96039-d5f3-4fd1-b87b-14b0ec6469fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "\n",
    "# Dividindo os tipos de arquivos em partes menores para facilitar o processamento\n",
    "tipos_arquivos_parte1 = [\"SinReg\", \"auto_cau\", \"auto_cat\"]\n",
    "tipos_arquivos_parte2 = [\"auto_cep\", \"auto_cidade\", \"auto_cob\"]\n",
    "tipos_arquivos_parte3 = [\"auto_idade\", \"auto_reg\", \"auto_sexo\"]\n",
    "tipos_arquivos_parte4 = [\"auto2_grupo\", \"auto2_vei\", \"PremReg\"]\n",
    "tipos_arquivos_parte5 = [\"arq_casco_comp\", \"arq_casco3_comp\", \"arq_casco4_comp\"]\n",
    "\n",
    "# Função para tratar valores ausentes nos DataFrames\n",
    "def tratar_valores_ausentes(df):\n",
    "    # Para colunas numéricas, preencher com a média\n",
    "    colunas_numericas = [c for c, t in df.dtypes if t in ('int', 'double')]\n",
    "    for coluna in colunas_numericas:\n",
    "        media = df.agg(mean(coluna)).collect()[0][0]\n",
    "        df = df.fillna({coluna: media})\n",
    "    \n",
    "    # Para colunas categóricas, usar o valor mais frequente\n",
    "    colunas_categoricas = [c for c, t in df.dtypes if t == 'string']\n",
    "    for coluna in colunas_categoricas:\n",
    "        valor_frequente = df.groupBy(coluna).count().orderBy('count', ascending=False).first()[0]\n",
    "        df = df.fillna({coluna: valor_frequente})\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Função para processar e salvar DataFrames de uma parte\n",
    "def processar_e_salvar(tipos_arquivos):\n",
    "    for tipo in tipos_arquivos:\n",
    "        nome_tabela_final = f\"dados_{tipo}_limpo\"\n",
    "        df = spark.sql(f\"SELECT * FROM {nome_tabela_final}\")\n",
    "        \n",
    "        # Tratando valores ausentes\n",
    "        df_tratado = tratar_valores_ausentes(df)\n",
    "        df_tratado.createOrReplaceTempView(nome_tabela_final + \"_tratado\")\n",
    "        \n",
    "        # Salvando o DataFrame tratado\n",
    "        caminho_arquivo = f\"dbfs:/FileStore/tables/{tipo}_tratado.parquet\"\n",
    "        df_tratado.write.mode(\"overwrite\").parquet(caminho_arquivo)\n",
    "        print(f\"Arquivo {tipo}_tratado.parquet salvo com sucesso.\")\n",
    "\n",
    "# Processar cada parte separadamente\n",
    "processar_e_salvar(tipos_arquivos_parte1)\n",
    "processar_e_salvar(tipos_arquivos_parte2)\n",
    "processar_e_salvar(tipos_arquivos_parte3)\n",
    "processar_e_salvar(tipos_arquivos_parte4)\n",
    "processar_e_salvar(tipos_arquivos_parte5)\n",
    "\n",
    "# Observação sobre arquivos que estão atrasando\n",
    "print(\"Os arquivos arq_casco3 e arq_casco4 estão atrasando o processamento, então decidi seguir sem esses DataFrames tratados, excluindo-os das análises.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5354daff-b6fc-42ad-9040-c9d1a819a82b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Criar a Função de Normalização de Texto e Aplicar a Função aos DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "fd177571-861c-4186-a902-81a429cd7cf1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower, trim, col\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Lista de tipos de arquivos divididos em partes\n",
    "tipos_arquivos_parte1 = [\"SinReg\", \"auto_cau\", \"auto_cat\"]\n",
    "tipos_arquivos_parte2 = [\"auto_cep\", \"auto_cidade\", \"auto_cob\"]\n",
    "tipos_arquivos_parte3 = [\"auto_idade\", \"auto_reg\", \"auto_sexo\"]\n",
    "tipos_arquivos_parte4 = [\"auto2_grupo\", \"auto2_vei\", \"PremReg\"]\n",
    "tipos_arquivos_parte5 = [\"arq_casco_comp\", \"arq_casco3_comp\", \"arq_casco4_comp\"]\n",
    "\n",
    "# Função UDF para normalizar texto\n",
    "def normalizar_texto(coluna):\n",
    "    \"\"\"\n",
    "    Esta função transforma o texto em minúsculas e remove espaços extras.\n",
    "    \"\"\"\n",
    "    return trim(lower(coluna))\n",
    "\n",
    "# Função para normalizar dados textuais em um DataFrame\n",
    "def normalizar_dados_textuais(df):\n",
    "    \"\"\"\n",
    "    Aplica a normalização a todas as colunas do tipo texto no DataFrame.\n",
    "    \"\"\"\n",
    "    colunas_textuais = [c for c, t in df.dtypes if t == 'string']\n",
    "    for coluna in colunas_textuais:\n",
    "        df = df.withColumn(coluna, normalizar_texto(col(coluna)))\n",
    "    return df\n",
    "\n",
    "# Função para carregar, normalizar e salvar DataFrames de texto\n",
    "def carregar_normalizar_e_salvar_textuais(tipos_arquivos):\n",
    "    for tipo in tipos_arquivos:\n",
    "        caminho_parquet = f\"dbfs:/FileStore/tables/{tipo}_concatenado.parquet\"\n",
    "        \n",
    "        # Carregar o DataFrame do arquivo Parquet\n",
    "        df_concatenado = spark.read.format(\"parquet\").load(caminho_parquet)\n",
    "        \n",
    "        # Normalizar o DataFrame carregado\n",
    "        df_normalizado = normalizar_dados_textuais(df_concatenado)\n",
    "        \n",
    "        # Criar uma view temporária com os dados normalizados\n",
    "        nome_tabela_final = f\"dados_{tipo}_textual_normalizado\"\n",
    "        df_normalizado.createOrReplaceTempView(nome_tabela_final)\n",
    "        \n",
    "        # Salvar o DataFrame normalizado em um novo arquivo Parquet\n",
    "        caminho_arquivo = f\"dbfs:/FileStore/tables/{tipo}_textual_normalizado.parquet\"\n",
    "        df_normalizado.write.mode(\"overwrite\").parquet(caminho_arquivo)\n",
    "        \n",
    "        print(f\"Arquivo {tipo}_textual_normalizado.parquet salvo com sucesso.\")\n",
    "\n",
    "# Processar e salvar cada parte dos DataFrames\n",
    "carregar_normalizar_e_salvar_textuais(tipos_arquivos_parte1)\n",
    "carregar_normalizar_e_salvar_textuais(tipos_arquivos_parte2)\n",
    "carregar_normalizar_e_salvar_textuais(tipos_arquivos_parte3)\n",
    "carregar_normalizar_e_salvar_textuais(tipos_arquivos_parte4)\n",
    "carregar_normalizar_e_salvar_textuais(tipos_arquivos_parte5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc0f0938-0c14-451a-a057-77893642d829",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Identificação e Tratamento de Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "269bbf38-b00f-4a70-b9b0-1f4f91120c57",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Função para tratar outliers em DataFrames\n",
    "def tratar_outliers(df):\n",
    "    \"\"\"\n",
    "    Identifica e trata outliers em variáveis numéricas utilizando o método do intervalo interquartil (IQR).\n",
    "    \"\"\"\n",
    "    # Selecionar colunas numéricas do DataFrame\n",
    "    colunas_numericas = [c for c, t in df.dtypes if t in ('int', 'double')]\n",
    "    for coluna in colunas_numericas:\n",
    "        # Calcular os quantis para determinar o IQR\n",
    "        quantile1 = df.approxQuantile(coluna, [0.25], 0.05)[0]\n",
    "        quantile3 = df.approxQuantile(coluna, [0.75], 0.05)[0]\n",
    "        iqr = quantile3 - quantile1\n",
    "        limite_inferior = quantile1 - 1.5 * iqr\n",
    "        limite_superior = quantile3 + 1.5 * iqr\n",
    "\n",
    "        # Filtrar o DataFrame para remover os outliers\n",
    "        df = df.filter((col(coluna) >= limite_inferior) & (col(coluna) <= limite_superior))\n",
    "\n",
    "    return df\n",
    "\n",
    "# Função para processar e salvar DataFrames sem outliers\n",
    "def processar_e_salvar_outliers(tipos_arquivos):\n",
    "    for tipo in tipos_arquivos:\n",
    "        nome_tabela_tratado = f\"dados_{tipo}_tratado\"\n",
    "        \n",
    "        # Verificar se a tabela tratada já existe\n",
    "        if nome_tabela_tratado in [t.name for t in spark.catalog.listTables()]:\n",
    "            df = spark.sql(f\"SELECT * FROM {nome_tabela_tratado}\")\n",
    "            df_sem_outliers = tratar_outliers(df)\n",
    "            df_sem_outliers.createOrReplaceTempView(nome_tabela_tratado + \"_sem_outliers\")\n",
    "\n",
    "            # Salvar o DataFrame sem outliers em formato Parquet\n",
    "            caminho_arquivo = f\"dbfs:/FileStore/tables/{tipo}_sem_outliers.parquet\"\n",
    "            df_sem_outliers.write.mode(\"overwrite\").parquet(caminho_arquivo)\n",
    "            print(f\"Arquivo {tipo}_sem_outliers.parquet salvo com sucesso.\")\n",
    "        else:\n",
    "            print(f\"Tabela {nome_tabela_tratado} não encontrada.\")\n",
    "\n",
    "# Listas de tipos de arquivos divididos em partes\n",
    "tipos_arquivos_parte1 = [\"SinReg\", \"auto_cau\", \"auto_cat\"]\n",
    "tipos_arquivos_parte2 = [\"auto_cep\", \"auto_cidade\", \"auto_cob\"]\n",
    "tipos_arquivos_parte3 = [\"auto_idade\", \"auto_reg\", \"auto_sexo\"]\n",
    "tipos_arquivos_parte4 = [\"auto2_grupo\", \"auto2_vei\", \"PremReg\"]\n",
    "tipos_arquivos_parte5 = [\"arq_casco_comp\", \"arq_casco3_comp\", \"arq_casco4_comp\"]\n",
    "\n",
    "# Processar e salvar DataFrames sem outliers para cada parte\n",
    "processar_e_salvar_outliers(tipos_arquivos_parte1)\n",
    "processar_e_salvar_outliers(tipos_arquivos_parte2)\n",
    "processar_e_salvar_outliers(tipos_arquivos_parte3)\n",
    "processar_e_salvar_outliers(tipos_arquivos_parte4)\n",
    "processar_e_salvar_outliers(tipos_arquivos_parte5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75e0152b-b8f0-49d6-aed0-b3d554e2844a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Conversão de Variáveis Categóricas em Variáveis Numéricas (One-Hot Encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "4a9bf109-3706-4497-a368-98e840fdbba4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Função para fazer One-Hot Encoding em colunas categóricas\n",
    "def one_hot_encoding(df):\n",
    "    # Identificar colunas categóricas\n",
    "    colunas_categoricas = [c for c in df.columns if df.schema[c].dataType == StringType()]\n",
    "    if len(colunas_categoricas) > 0:  # Verifica se há colunas categóricas\n",
    "        # Criar os indexadores e codificadores\n",
    "        indexers = [StringIndexer(inputCol=c, outputCol=c + \"_index\") for c in colunas_categoricas]\n",
    "        encoders = [OneHotEncoder(inputCols=[c + \"_index\"], outputCols=[c + \"_vec\"]) for c in colunas_categoricas]\n",
    "        \n",
    "        # Montar o pipeline\n",
    "        pipeline = Pipeline(stages=indexers + encoders)\n",
    "        model = pipeline.fit(df)  # Ajustar o modelo ao DataFrame\n",
    "        df_codificado = model.transform(df)  # Transformar o DataFrame\n",
    "\n",
    "        # Remover colunas categóricas originais e suas versões indexadas\n",
    "        colunas_remover = colunas_categoricas + [c + \"_index\" for c in colunas_categoricas]\n",
    "        df_codificado = df_codificado.drop(*colunas_remover)\n",
    "\n",
    "        return df_codificado  # Retornar o DataFrame codificado\n",
    "    else:\n",
    "        return df  # Se não houver colunas categóricas, retornar o DataFrame original\n",
    "\n",
    "# Função para processar e salvar DataFrames com One-Hot Encoding\n",
    "def processar_e_salvar_codificacao(tipos_arquivos):\n",
    "    for tipo in tipos_arquivos:\n",
    "        caminho_parquet = f\"dbfs:/FileStore/tables/{tipo}_sem_outliers.parquet\"\n",
    "        \n",
    "        try:\n",
    "            df = spark.read.format(\"parquet\").load(caminho_parquet)  # Carregar DataFrame\n",
    "            print(f\"Tabela {tipo}_sem_outliers carregada com sucesso.\")\n",
    "            \n",
    "            # Processar apenas um subconjunto se a tabela for muito grande, devido ao tempo de duração do cluster\n",
    "            if tipo == \"arq_casco_comp\":\n",
    "                df = df.limit(100000)  # Limitar o número de registros\n",
    "            \n",
    "            # Aplicar One-Hot Encoding\n",
    "            df_codificado = one_hot_encoding(df)\n",
    "            df_codificado.createOrReplaceTempView(f\"dados_{tipo}_sem_outliers_codificado\")\n",
    "            \n",
    "            # Salvar o DataFrame codificado\n",
    "            caminho_arquivo_codificado = f\"dbfs:/FileStore/tables/{tipo}_codificado.parquet\"\n",
    "            df_codificado.write.mode(\"overwrite\").parquet(caminho_arquivo_codificado)\n",
    "            print(f\"Arquivo {tipo}_codificado.parquet salvo com sucesso.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao carregar ou processar a tabela {tipo}: {e}\")\n",
    "\n",
    "# Processar e salvar DataFrames codificados para cada parte\n",
    "processar_e_salvar_codificacao(tipos_arquivos_parte1)\n",
    "processar_e_salvar_codificacao(tipos_arquivos_parte2)\n",
    "processar_e_salvar_codificacao(tipos_arquivos_parte3)\n",
    "processar_e_salvar_codificacao(tipos_arquivos_parte4)\n",
    "processar_e_salvar_codificacao(tipos_arquivos_parte5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9717220-0baf-4cef-98f1-48cded9dc258",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Normalização de Variáveis Numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a5ad79db-cfd9-4595-844d-e5b1f5906afe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.utils\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Função para normalizar variáveis numéricas\n",
    "def normalizar_variaveis_numericas(df):\n",
    "    # Selecionar colunas numéricas\n",
    "    numeric_columns = [field.name for field in df.schema.fields if field.dataType.simpleString() in ['double', 'float', 'int']]\n",
    "    \n",
    "    # Normalização usando StandardScaler\n",
    "    for column in numeric_columns:\n",
    "        # Criar vetor para a coluna\n",
    "        assembler = VectorAssembler(inputCols=[column], outputCol=f\"{column}_vector\")\n",
    "        scaler = StandardScaler(inputCol=f\"{column}_vector\", outputCol=f\"{column}_scaled\", withMean=True, withStd=True)\n",
    "        pipeline = Pipeline(stages=[assembler, scaler])\n",
    "        \n",
    "        # Aplicar o pipeline para normalização\n",
    "        df = pipeline.fit(df).transform(df).drop(f\"{column}_vector\")  # Remove a coluna vetor após normalizar\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Função principal para processar e salvar a normalização\n",
    "def processar_e_salvar_normalizacao(tipos_arquivos):\n",
    "    caminho_diretorio_normalizados = \"dbfs:/FileStore/tables/numerico_normalizados\"\n",
    "    \n",
    "    # Criar diretório se não existir\n",
    "    if not dbutils.fs.ls(caminho_diretorio_normalizados):\n",
    "        dbutils.fs.mkdirs(caminho_diretorio_normalizados)\n",
    "        print(f\"Diretório {caminho_diretorio_normalizados} criado.\")\n",
    "    else:\n",
    "        print(f\"Diretório {caminho_diretorio_normalizados} já existe.\")\n",
    "    \n",
    "    for tipo in tipos_arquivos:\n",
    "        caminho_parquet = f\"dbfs:/FileStore/tables/{tipo}_sem_outliers.parquet\"\n",
    "        \n",
    "        try:\n",
    "            # Carregar o DataFrame\n",
    "            df = spark.read.format(\"parquet\").load(caminho_parquet)\n",
    "            print(f\"Tabela {tipo}_sem_outliers carregada com sucesso.\")\n",
    "            \n",
    "            # Normalizar as variáveis numéricas\n",
    "            df_normalizado = normalizar_variaveis_numericas(df)\n",
    "            df_normalizado.createOrReplaceTempView(f\"dados_{tipo}_sem_outliers_numerico_normalizado\")\n",
    "\n",
    "            # Mostrar esquema e primeiras 5 linhas\n",
    "            df_normalizado.printSchema()\n",
    "            df_normalizado.show(5)\n",
    "\n",
    "            # Salvar DataFrame normalizado\n",
    "            caminho_arquivo_normalizado = f\"{caminho_diretorio_normalizados}/{tipo}_numerico_normalizado.parquet\"\n",
    "            print(f\"Tentando salvar {tipo}_numerico_normalizado.parquet...\")\n",
    "            df_normalizado.write.mode(\"overwrite\").parquet(caminho_arquivo_normalizado)\n",
    "            print(f\"Arquivo {tipo}_numerico_normalizado.parquet salvo com sucesso.\")\n",
    "        \n",
    "        # Verificação de erros\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Erro: Arquivo {caminho_parquet} não encontrado para {tipo}. {e}\")\n",
    "        except pyspark.sql.utils.AnalysisException as e:\n",
    "            print(f\"Erro de análise ao processar a tabela {tipo}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao processar a tabela {tipo}: {e}\")\n",
    "\n",
    "# Processar e salvar DataFrames normalizados para cada parte\n",
    "processar_e_salvar_normalizacao(tipos_arquivos_parte1)\n",
    "processar_e_salvar_normalizacao(tipos_arquivos_parte2)\n",
    "processar_e_salvar_normalizacao(tipos_arquivos_parte3)\n",
    "processar_e_salvar_normalizacao(tipos_arquivos_parte4)\n",
    "processar_e_salvar_normalizacao(tipos_arquivos_parte5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13d4356f-0f21-427e-9bfe-d74a44cc540b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Executar DataFrames tratados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "fdf3da2c-abd8-4520-a066-fb6dcaddd480",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Embora todos os DataFrames sejam carregados, somente o DataFrame com a versão \"_tratado\" contém todas as alterações realizadas anteriormente.\n",
    "\n",
    "# Lista de tipos de arquivos divididos em partes\n",
    "tipos_arquivos_parte1 = [\"SinReg\", \"auto_cau\", \"auto_cat\"]\n",
    "tipos_arquivos_parte2 = [\"auto_cep\", \"auto_cidade\", \"auto_cob\"]\n",
    "tipos_arquivos_parte3 = [\"auto_idade\", \"auto_reg\", \"auto_sexo\"]\n",
    "tipos_arquivos_parte4 = [\"auto2_grupo\", \"auto2_vei\", \"PremReg\"]\n",
    "tipos_arquivos_parte5 = [\"arq_casco_comp\", \"arq_casco3_comp\", \"arq_casco4_comp\"]\n",
    "\n",
    "# Lista das versões dos arquivos a serem carregados\n",
    "versoes = [\"tratado\", \"codificado\", \"sem_outliers\", \"textual_normalizado\", \"normalizado\"]\n",
    "\n",
    "# Função para carregar e exibir DataFrames\n",
    "def carregar_e_exibir(tipos_arquivos, versao):\n",
    "    for tipo in tipos_arquivos:\n",
    "        # Montar o caminho do arquivo Parquet para a versão específica\n",
    "        caminho_arquivo = f\"dbfs:/FileStore/tables/{tipo}_{versao}.parquet\"\n",
    "        \n",
    "        # Tentar carregar o DataFrame do arquivo Parquet\n",
    "        try:\n",
    "            df = spark.read.parquet(caminho_arquivo)\n",
    "            nome_tabela_final = f\"{tipo}_{versao}\"\n",
    "            df.createOrReplaceTempView(nome_tabela_final)\n",
    "            \n",
    "            # Mostrar as primeiras 50 linhas do DataFrame carregado\n",
    "            print(f\"\\nTabela: {nome_tabela_final}\")\n",
    "            consulta = f\"SELECT * FROM {nome_tabela_final} LIMIT 50\"\n",
    "            resultado = spark.sql(consulta)\n",
    "            resultado.display()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Não foi possível carregar {caminho_arquivo}: {e}\")\n",
    "\n",
    "# Função para carregar e exibir todas as versões\n",
    "def carregar_por_versao(tipos_arquivos):\n",
    "    for versao in versoes:\n",
    "        print(f\"\\nCarregando arquivos da versão: {versao}\")\n",
    "        carregar_e_exibir(tipos_arquivos, versao)\n",
    "\n",
    "# Carregar e exibir DataFrames para cada parte e versão\n",
    "carregar_por_versao(tipos_arquivos_parte1)\n",
    "carregar_por_versao(tipos_arquivos_parte2)\n",
    "carregar_por_versao(tipos_arquivos_parte3)\n",
    "carregar_por_versao(tipos_arquivos_parte4)\n",
    "carregar_por_versao(tipos_arquivos_parte5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7295380-a44e-49ae-8fa4-1ebe94709e37",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Feature Engineering: Derivação de Variáveis de Risco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1c14cff6-456d-45c6-90c0-b9c059503e5b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Verificar categorias\n",
    "SELECT DISTINCT CODIGO, CATEGORIA\n",
    "FROM auto_cat_tratado;\n",
    "\n",
    "-- Modelo do Veículo\n",
    "SELECT *,\n",
    "       CASE\n",
    "           WHEN codigo = 1 THEN 'Automóveis Comuns (Passeio Nacional)'\n",
    "           WHEN codigo = 2 THEN 'Automóveis Importados'\n",
    "           WHEN codigo = 3 THEN 'Pick-ups'\n",
    "           WHEN codigo IN (3, 4) THEN 'Veículos de Carga (nacional e importado)'\n",
    "           WHEN codigo = 5 THEN 'Motocicletas (nacional e importado)'\n",
    "           WHEN codigo = 6 THEN 'Ônibus (nacional e importado)'\n",
    "           WHEN codigo = 7 THEN 'Utilitários (nacional e importado)'\n",
    "           WHEN codigo = 9 THEN 'Outros'\n",
    "           ELSE 'Categoria Desconhecida'\n",
    "       END AS grupo_de_risco\n",
    "FROM auto_cat_tratado;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a40c6982-31d0-48dd-83fc-6f77f2f9b827",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Consulta para mostrar as colunas da tabela arq_casco_comp_tratado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f56c005-5825-4043-98f5-206dd96e833a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM arq_casco_comp_tratado LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f418c45-cbfc-4054-99b2-6b97db72fe8c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Cálculo da Idade do Veículo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7842f8e7-7603-4101-bf8b-095cc3d7dbfb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SET ano_atual = 2024;\n",
    "SELECT *,\n",
    "       2024 - ANO_MODELO AS Idade_Veiculo\n",
    "FROM arq_casco_comp_tratado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1bf8f06-ddcd-40d4-ab4f-2247a3d44fee",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Valor Segurado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3700c638-2c3f-4f5f-9315-ecc87ff8fd04",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT IS_MEDIA, EXPOSICAO1, PREMIO1\n",
    "FROM arq_casco_comp_tratado\n",
    "LIMIT 50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ea67635-324d-4ea1-91cc-5e0cf092e732",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Categorização da Idade do Segurado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18cb1d1e-2398-49da-b56a-de4962cb3b9b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "    TRIM(descricao) AS descricao, -- remover espaços em branco do início e do final de uma string\n",
    "    CASE \n",
    "        WHEN TRIM(descricao) = 'Entre 18 e 25 anos' THEN '18-25'\n",
    "        WHEN TRIM(descricao) = 'Entre 26 e 35 anos' THEN '26-35'\n",
    "        WHEN TRIM(descricao) = 'Entre 36 e 45 anos' THEN '36-45'\n",
    "        WHEN TRIM(descricao) = 'Entre 46 e 55 anos' THEN '46-55'\n",
    "        WHEN TRIM(descricao) = 'Maior que 55 anos' THEN '56 ou +'\n",
    "        WHEN TRIM(descricao) = 'N�o informada' THEN 'Não informada'\n",
    "        ELSE 'Desconhecido'\n",
    "    END AS Faixa_Etaria,\n",
    "    COUNT(*) AS Total_Segurados\n",
    "FROM \n",
    "    auto_idade_tratado\n",
    "GROUP BY \n",
    "    TRIM(descricao)\n",
    "LIMIT 50;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88789988-9dc1-425d-9f38-017dbe657227",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Índice de Risco por Cidade ou Estado\n",
    "- Roubo e Furto\n",
    "- Colisão Parcial\n",
    "- Perda Total\n",
    "- Incêndio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "018222c5-8f57-42ff-a2cb-1b79506b641a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "    REGIAO,\n",
    "    COUNT(*) AS FREQ_SIN1, -- Total de sinistros da cobertura roubo/furto\n",
    "    SUM(INDENIZ1) AS Total_Indenizacoes_roubo_furto -- Total de indenizações de sinistros da cobertura roubo/furto\n",
    "FROM \n",
    "    arq_casco_comp_tratado\n",
    "WHERE \n",
    "    INDENIZ1 > 0 -- Filtrar apenas os sinistros que resultaram em indenizações\n",
    "GROUP BY \n",
    "    REGIAO\n",
    "ORDER BY \n",
    "    Total_Indenizacoes_roubo_furto DESC\n",
    "LIMIT 50;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15ddf7ad-7c64-409e-903b-61a8de72b130",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "    REGIAO,\n",
    "    COUNT(*) AS FREQ_SIN2, -- Quantidade de sinistros da cobertura colisão parcial\n",
    "    SUM(INDENIZ2) AS Total_Indenizacoes_Colisao_Parcial -- Total de indenizações de sinistros da cobertura colisão parcial\n",
    "FROM \n",
    "    arq_casco_comp_tratado\n",
    "WHERE \n",
    "    INDENIZ2 > 0 -- Filtrar apenas os sinistros que resultaram em indenizações\n",
    "GROUP BY \n",
    "    REGIAO\n",
    "ORDER BY \n",
    "    Total_Indenizacoes_Colisao_Parcial DESC\n",
    "LIMIT 50;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9cf0c9dd-c860-4d9b-a2fd-0f453089b243",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "    REGIAO,\n",
    "    COUNT(*) AS FREQ_SIN3, -- - Quantidade de sinistros da cobertura colisão perda total\n",
    "    SUM(INDENIZ3) AS Total_Indenizacoes_perda_total -- Total de indenizações de sinistros da cobertura colisão perda total\n",
    "FROM \n",
    "    arq_casco_comp_tratado\n",
    "WHERE \n",
    "    INDENIZ3 > 0 -- Filtrar apenas os sinistros que resultaram em indenizações\n",
    "GROUP BY \n",
    "    REGIAO\n",
    "ORDER BY \n",
    "    Total_Indenizacoes_perda_total DESC\n",
    "LIMIT 50;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b68863ee-f13f-4c90-8b01-dc041cb82cee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "    REGIAO,\n",
    "    COUNT(*) AS FREQ_SIN4, -- Quantidade de sinistros da cobertura incêndio\n",
    "    SUM(INDENIZ4) AS Total_indenizacoes_incendio -- Total de indenizações de sinistros da cobertura incêndio\n",
    "FROM \n",
    "    arq_casco_comp_tratado\n",
    "WHERE \n",
    "    INDENIZ4 > 0 -- Filtrar apenas os sinistros que resultaram em indenizações\n",
    "GROUP BY \n",
    "    REGIAO\n",
    "ORDER BY \n",
    "    Total_indenizacoes_incendio DESC\n",
    "LIMIT 50;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "556b650b-ad35-4cba-b021-946f299b3dd6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Razão Prêmio/Valor Segurado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3174e784-a2ef-4462-8f76-54a22801d0fc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "    REGIAO,\n",
    "    SUM(PREMIO1) AS Total_Premio, -- Total de prêmios\n",
    "    SUM(IS_MEDIA * EXPOSICAO1) AS Total_Valor_Segurado, -- Total dos valores segurados ponderados\n",
    "    (SUM(PREMIO1) / NULLIF(SUM(IS_MEDIA * EXPOSICAO1), 0)) AS Premio_Valor_Segurado -- Razão Prêmio/Valor Segurado\n",
    "FROM \n",
    "    arq_casco_comp_tratado\n",
    "GROUP BY \n",
    "    REGIAO\n",
    "ORDER BY \n",
    "    Premio_Valor_Segurado DESC\n",
    "LIMIT 50;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45e56bc1-9db0-41f9-b631-1cce69b0540b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Frequência de Sinistros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a3a0355-73f1-4bc9-ac63-0675a3494bbc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "    REGIAO,\n",
    "    SUM(FREQ_SIN1 + FREQ_SIN2 + FREQ_SIN3 + FREQ_SIN4 + FREQ_SIN9) AS Numero_Sinistros,\n",
    "    SUM(EXPOSICAO1) AS Tempo_Exposicao,\n",
    "    SUM(FREQ_SIN1 + FREQ_SIN2 + FREQ_SIN3 + FREQ_SIN4 + FREQ_SIN9) / NULLIF(SUM(EXPOSICAO1), 0) AS Frequencia_Sinistros\n",
    "FROM \n",
    "    arq_casco_comp_tratado\n",
    "GROUP BY \n",
    "    REGIAO\n",
    "ORDER BY \n",
    "    Frequencia_Sinistros DESC\n",
    "LIMIT 50;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6edd7b78-e488-478d-8f4e-7193a9159554",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Severidade Média dos Sinistros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a67855a2-1678-4bc2-bb38-e2ebbb17bf2f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "    REGIAO,\n",
    "    SUM(INDENIZ1 + INDENIZ2 + INDENIZ3 + INDENIZ4 + INDENIZ9) AS Valor_Indenizacao, -- Total das indenizações\n",
    "    SUM(FREQ_SIN1 + FREQ_SIN2 + FREQ_SIN3 + FREQ_SIN4 + FREQ_SIN9) AS Numero_Sinistros, -- Total de sinistros\n",
    "    CASE \n",
    "        WHEN SUM(FREQ_SIN1 + FREQ_SIN2 + FREQ_SIN3 + FREQ_SIN4 + FREQ_SIN9) = 0 THEN 0\n",
    "        ELSE SUM(INDENIZ1 + INDENIZ2 + INDENIZ3 + INDENIZ4 + INDENIZ9) / NULLIF(SUM(FREQ_SIN1 + FREQ_SIN2 + FREQ_SIN3 + FREQ_SIN4 + FREQ_SIN9), 0) \n",
    "    END AS Severidade_Sinistros -- Severidade média dos sinistros\n",
    "FROM \n",
    "    arq_casco_comp_tratado\n",
    "GROUP BY \n",
    "    REGIAO\n",
    "ORDER BY \n",
    "    Severidade_Sinistros DESC\n",
    "LIMIT 50;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "687c54f3-a5ae-47a3-bd0d-48e8369823f9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Análise Exploratória de Dados (EDA)\n",
    "- Análise Estatística Descritiva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2aa5138-64ec-4dec-b81a-80f92e737d52",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Carregar bibliotecas\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Carregar os dados do Spark em um DataFrame do pandas\n",
    "arq_casco_comp_tratado = spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM arq_casco_comp_tratado\n",
    "WHERE COD_TARIF = 1  -- Apenas veículos de passeio\n",
    "  AND ANO_MODELO > 1978 AND ANO_MODELO <= 2024\n",
    "\"\"\").toPandas()\n",
    "\n",
    "# 1. Verificar as primeiras linhas do DataFrame\n",
    "print(arq_casco_comp_tratado.head())\n",
    "\n",
    "# 2. Informações gerais do DataFrame\n",
    "print(arq_casco_comp_tratado.info())\n",
    "\n",
    "# 3. Estatísticas descritivas\n",
    "print(arq_casco_comp_tratado.describe())\n",
    "\n",
    "# 4. Verificar valores ausentes\n",
    "print(arq_casco_comp_tratado.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9bbb070-1a4e-4e63-8778-8743115f8b6d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Identificação de Fatores de Risco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1fdf26c2-d1a2-49d3-8c52-4c0a970b8fb9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Valor Total das Indenizações por Categoria de Veículo\n",
    "CREATE OR REPLACE TEMP VIEW total_indenizacoes_categoria AS\n",
    "SELECT \n",
    "    a.CATEGORIA,  -- Categoria do veículo\n",
    "    SUM(c.INDENIZ1) AS Total_Indenizacoes_Roubo_Furto, \n",
    "    SUM(c.INDENIZ2) AS Total_Indenizacoes_Colisao_Parcial,  \n",
    "    SUM(c.INDENIZ3) AS Total_Indenizacoes_Colisao_Perdida_Total,  \n",
    "    SUM(c.INDENIZ4) AS Total_Indenizacoes_Incendio,  \n",
    "    SUM(c.INDENIZ9) AS Total_Indenizacoes_Outras,\n",
    "    SUM(c.INDENIZ1 + c.INDENIZ2 + c.INDENIZ3 + c.INDENIZ4 + c.INDENIZ9) AS Total_Indenizacoes_Geral  -- Total geral de indenizações\n",
    "FROM \n",
    "    arq_casco_comp_tratado c  -- Tabela de sinistros\n",
    "JOIN \n",
    "    auto_cat_tratado a ON c.COD_TARIF = a.CODIGO  -- Junta com a tabela de categorias\n",
    "GROUP BY \n",
    "    a.CATEGORIA;  -- Agrupa pelos tipos de veículos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9fa00a0f-c032-4dfe-a725-fe631fa2847e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Análise do Risco de Incêndio em Veículos: Relação entre Veiculo_Idade, Região e Sinistros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc32f7a9-2016-4c0d-b31d-fa97b34ee0c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Criando uma visualização temporária para analisar o risco de incêndio por idade do veículo e região\n",
    "CREATE OR REPLACE TEMP VIEW analise_risco_incendio AS\n",
    "WITH Veiculo_Idade AS (\n",
    "    SELECT v.*,\n",
    "           CASE \n",
    "               WHEN v.ANO_MODELO = 2024 THEN 0 -- Se o ano do modelo é 2024, idade é 0\n",
    "               ELSE 2024 - v.ANO_MODELO -- Caso contrário, calcula a idade normalmente\n",
    "           END AS Idade_Veiculo\n",
    "    FROM arq_casco_comp_tratado v\n",
    "    JOIN auto_cat_tratado a ON v.COD_TARIF = a.CODIGO\n",
    "    WHERE v.ANO_MODELO > 1978 AND v.ANO_MODELO <= 2024 -- Filtrando anos válidos\n",
    "      AND a.CODIGO = 1  -- Apenas veículos de passeio\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    vi.Idade_Veiculo,\n",
    "    si.REGIAO,\n",
    "    COUNT(*) AS Freq_Sinistros_Incendio,\n",
    "    SUM(si.INDENIZ4) AS Total_Indenizacoes_Incendio\n",
    "FROM \n",
    "    Veiculo_Idade vi\n",
    "JOIN \n",
    "    arq_casco_comp_tratado si ON vi.REGIAO = si.REGIAO\n",
    "WHERE \n",
    "    si.INDENIZ4 > 0 -- Filtra apenas os sinistros de incêndio\n",
    "GROUP BY \n",
    "    vi.Idade_Veiculo, si.REGIAO\n",
    "ORDER BY \n",
    "    vi.Idade_Veiculo, Total_Indenizacoes_Incendio DESC;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "afde40a5-010a-440e-8f64-e4c3e5934cab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Gráfico de Sinistros de Incêndio por Região\n",
    "# Carregar bibliotecas\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Carregar os resultados da consulta SQL para um DataFrame\n",
    "result_df_regiao = spark.sql(\"\"\"\n",
    "SELECT REGIAO, SUM(INDENIZ4) AS Freq_Sinistros_Incendio\n",
    "FROM arq_casco_comp_tratado\n",
    "WHERE COD_TARIF = 1  -- Apenas veículos de passeio\n",
    "  AND ANO_MODELO > 1978 AND ANO_MODELO <= 2024\n",
    "GROUP BY REGIAO\n",
    "HAVING SUM(INDENIZ4) > 0  -- Filtrar apenas regiões com sinistros\n",
    "\"\"\").toPandas()\n",
    "\n",
    "# Criar o gráfico\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=result_df_regiao, x='REGIAO', y='Freq_Sinistros_Incendio')\n",
    "plt.title('Frequência de Sinistros por Incêndio por Região')\n",
    "plt.xlabel('Região')\n",
    "plt.ylabel('Frequência de Sinistros por Incêndio')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "80b8f9d0-89fd-4c65-b5e8-e85f569f4905",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Gráfico de Sinistros de Incêndio por Idade do Veículo\n",
    "# Carregar resultados para um DataFrame\n",
    "result_df_idade = spark.sql(\"\"\"\n",
    "WITH Veiculo_Idade AS (\n",
    "    SELECT v.ANO_MODELO,\n",
    "           (2024 - v.ANO_MODELO) AS Idade_Veiculo,\n",
    "           SUM(v.INDENIZ4) AS Freq_Sinistros_Incendio\n",
    "    FROM arq_casco_comp_tratado v\n",
    "    JOIN auto_cat_tratado a ON v.COD_TARIF = a.CODIGO\n",
    "    WHERE v.ANO_MODELO > 1978 AND v.ANO_MODELO <= 2024\n",
    "      AND a.CODIGO = 1  -- Apenas veículos de passeio\n",
    "    GROUP BY v.ANO_MODELO\n",
    ")\n",
    "\n",
    "SELECT Idade_Veiculo, SUM(Freq_Sinistros_Incendio) AS Freq_Sinistros_Incendio\n",
    "FROM Veiculo_Idade\n",
    "GROUP BY Idade_Veiculo\n",
    "\"\"\").toPandas()\n",
    "\n",
    "# Criar o gráfico\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=result_df_idade, x='Idade_Veiculo', y='Freq_Sinistros_Incendio')\n",
    "plt.title('Frequência de Sinistros por Incêndio em Relação à Idade do Veículo')\n",
    "plt.xlabel('Idade do Veículo (anos)')\n",
    "plt.ylabel('Frequência de Sinistros por Incêndio')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8310b949-b150-4f31-98c6-8550fcf7bad4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Análise de Risco por Idade do Veículo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b3b5cca-4e38-4d0a-9287-07c471ce4acf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMP VIEW analise_risco AS\n",
    "WITH Veiculo_Idade AS (\n",
    "    SELECT v.*,\n",
    "           CASE \n",
    "               WHEN v.ANO_MODELO = 2024 THEN 0 -- 2024 - 0 = 2024, THEN TURN TO 0\n",
    "               ELSE 2024 - v.ANO_MODELO \n",
    "           END AS Idade_Veiculo\n",
    "    FROM arq_casco_comp_tratado v\n",
    "    JOIN auto_cat_tratado a ON v.COD_TARIF = a.CODIGO\n",
    "    WHERE v.ANO_MODELO > 1978 AND v.ANO_MODELO <= 2024 -- Filtrar possíveis erros\n",
    "      AND a.CODIGO = 1  -- Filtra apenas veículos de passeio (Tipo 1)\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    ANO_MODELO,\n",
    "    (FLOOR(Idade_Veiculo / 3) * 3) AS Faixa_Idade_Veiculo, -- Definir faixa para melhor visualização gráfica\n",
    "    COUNT(*) AS Frequencia_Sinistros,\n",
    "    SUM(INDENIZ1) AS Total_Indenizacoes_Roubo_Furto,\n",
    "    SUM(INDENIZ2) AS Total_Indenizacoes_Colisao_Parcial,\n",
    "    SUM(INDENIZ3) AS Total_Indenizacoes_Colisao_Perdida_Total,\n",
    "    SUM(INDENIZ4) AS Total_Indenizacoes_Incendio,\n",
    "    SUM(INDENIZ9) AS Total_Indenizacoes_Outras\n",
    "FROM \n",
    "    Veiculo_Idade\n",
    "WHERE \n",
    "    (INDENIZ1 IS NOT NULL OR \n",
    "     INDENIZ2 IS NOT NULL OR \n",
    "     INDENIZ3 IS NOT NULL OR \n",
    "     INDENIZ4 IS NOT NULL OR \n",
    "     INDENIZ9 IS NOT NULL)\n",
    "GROUP BY \n",
    "    ANO_MODELO, Faixa_Idade_Veiculo\n",
    "ORDER BY \n",
    "    Faixa_Idade_Veiculo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ce19e315-a1fa-4591-88e9-4c100562eadf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Frequência de Sinistros por Faixa de Idade do Veículo\n",
    "# Carregar bibliotecas\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Carregar os resultados da consulta SQL para um DataFrame\n",
    "result_df = spark.sql(\"SELECT * FROM analise_risco\").toPandas()\n",
    "\n",
    "# Criar o gráfico\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=result_df, x='Faixa_Idade_Veiculo', y='Frequencia_Sinistros')\n",
    "plt.title('Frequência de Sinistros por Faixa de Idade do Veículo')\n",
    "plt.xlabel('Faixa de Idade do Veículo (anos)')\n",
    "plt.ylabel('Frequência de Sinistros')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e7b2acb-1c6f-47c0-a15b-af5936c599ce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Indenizações por Faixa de Idade_Veiculo\n",
    "# Carregar bibliotecas\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Carregar os resultados da consulta SQL para um DataFrame\n",
    "result_df = spark.sql(\"SELECT * FROM analise_risco\").toPandas()\n",
    "\n",
    "# Definir tamanho do gráfico\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Gráfico para Roubo/Furto\n",
    "plt.subplot(3, 2, 1)\n",
    "sns.barplot(data=result_df, x='Faixa_Idade_Veiculo', y='Total_Indenizacoes_Roubo_Furto', color='blue')\n",
    "plt.title('Indenizações por Roubo/Furto')\n",
    "plt.xlabel('Faixa de Idade do Veículo (anos)')\n",
    "plt.ylabel('Total de Indenizações')\n",
    "\n",
    "# Gráfico para Colisão Parcial\n",
    "plt.subplot(3, 2, 2)\n",
    "sns.barplot(data=result_df, x='Faixa_Idade_Veiculo', y='Total_Indenizacoes_Colisao_Parcial', color='orange')\n",
    "plt.title('Indenizações por Colisão Parcial')\n",
    "plt.xlabel('Faixa de Idade do Veículo (anos)')\n",
    "plt.ylabel('Total de Indenizações')\n",
    "\n",
    "# Gráfico para Colisão Perdida Total\n",
    "plt.subplot(3, 2, 3)\n",
    "sns.barplot(data=result_df, x='Faixa_Idade_Veiculo', y='Total_Indenizacoes_Colisao_Perdida_Total', color='green')\n",
    "plt.title('Indenizações por Colisão Perdida Total')\n",
    "plt.xlabel('Faixa de Idade do Veículo (anos)')\n",
    "plt.ylabel('Total de Indenizações')\n",
    "\n",
    "# Gráfico para Incêndio\n",
    "plt.subplot(3, 2, 4)\n",
    "sns.barplot(data=result_df, x='Faixa_Idade_Veiculo', y='Total_Indenizacoes_Incendio', color='red')\n",
    "plt.title('Indenizações por Incêndio')\n",
    "plt.xlabel('Faixa de Idade do Veículo (anos)')\n",
    "plt.ylabel('Total de Indenizações')\n",
    "\n",
    "# Gráfico para Outras Indenizações\n",
    "plt.subplot(3, 2, 5)\n",
    "sns.barplot(data=result_df, x='Faixa_Idade_Veiculo', y='Total_Indenizacoes_Outras', color='purple')\n",
    "plt.title('Indenizações por Outras Categorias')\n",
    "plt.xlabel('Faixa de Idade do Veículo (anos)')\n",
    "plt.ylabel('Total de Indenizações')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "804665b6-de25-4019-999b-210f756f3255",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Análise de Risco por Região"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ff12149-a1a9-45ac-8ca1-4e8045681f55",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select distinct codigo, descricao from auto_reg_tratado\n",
    "order by codigo asc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b07aa335-8511-4b07-b50b-07a1c1cc7f2b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMP VIEW analise_regiao AS\n",
    "WITH Risco_Regiao AS (\n",
    "    SELECT \n",
    "        r.codigo AS Codigo_Regiao,\n",
    "        COUNT(c.INDENIZ1) AS Total_Sinistros_Roubo_Furto,  \n",
    "        COUNT(c.INDENIZ2) AS Total_Sinistros_Colisao_Parcial,  \n",
    "        COUNT(c.INDENIZ3) AS Total_Sinistros_Colisao_Perdida_Total,  \n",
    "        COUNT(c.INDENIZ4) AS Total_Sinistros_Incendio, \n",
    "        COUNT(c.INDENIZ9) AS Total_Sinistros_Outras, \n",
    "        SUM(c.INDENIZ1) AS Total_Indenizacoes_Roubo_Furto, \n",
    "        SUM(c.INDENIZ2) AS Total_Indenizacoes_Colisao_Parcial,  \n",
    "        SUM(c.INDENIZ3) AS Total_Indenizacoes_Colisao_Perdida_Total,  \n",
    "        SUM(c.INDENIZ4) AS Total_Indenizacoes_Incendio,  \n",
    "        SUM(c.INDENIZ9) AS Total_Indenizacoes_Outras  \n",
    "    FROM\n",
    "        arq_casco_comp_tratado c\n",
    "    JOIN \n",
    "        auto_reg_tratado r ON c.regiao = r.codigo\n",
    "    JOIN \n",
    "        auto_cat_tratado a ON c.COD_TARIF = a.CODIGO  \n",
    "    WHERE \n",
    "        a.CODIGO = 1  -- Filtra apenas veículos de passeio (Tipo 1)\n",
    "        AND c.idade <= 10  -- Filtra veículos com idade <= 10 anos\n",
    "    GROUP BY \n",
    "        r.codigo  \n",
    ")\n",
    "\n",
    "SELECT * FROM Risco_Regiao;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84cb2dd3-e1af-462b-9fcc-80e2cc1b7b36",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Total Geral de Sinistros por Região e Total por Tipo de Sinistros\n",
    "# Carregar bibliotecas\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Carregar os resultados da consulta SQL para um DataFrame\n",
    "result_df = spark.sql(\"SELECT * FROM analise_regiao\").toPandas()\n",
    "\n",
    "# Calcular o total de sinistros geral por região\n",
    "result_df['Total_Sinistros'] = (result_df['Total_Sinistros_Roubo_Furto'] + \n",
    "                                 result_df['Total_Sinistros_Colisao_Parcial'] + \n",
    "                                 result_df['Total_Sinistros_Colisao_Perdida_Total'] + \n",
    "                                 result_df['Total_Sinistros_Incendio'] + \n",
    "                                 result_df['Total_Sinistros_Outras'])\n",
    "\n",
    "# Criar gráficos para cada tipo de sinistro\n",
    "types_of_claims = [\n",
    "    'Total_Sinistros_Roubo_Furto',\n",
    "    'Total_Sinistros_Colisao_Parcial',\n",
    "    'Total_Sinistros_Colisao_Perdida_Total',\n",
    "    'Total_Sinistros_Incendio',\n",
    "    'Total_Sinistros_Outras'\n",
    "]\n",
    "\n",
    "# Configuração do gráfico\n",
    "plt.figure(figsize=(18, 12))\n",
    "\n",
    "# Loop para criar um gráfico para cada tipo de sinistro\n",
    "for i, claim_type in enumerate(types_of_claims, 1):\n",
    "    plt.subplot(3, 2, i)\n",
    "    sns.barplot(data=result_df, x='Codigo_Regiao', y=claim_type, palette='viridis')\n",
    "    plt.title(f'Total de {claim_type.replace(\"_\", \" \").title()} por Região')\n",
    "    plt.xlabel('Código da Região')\n",
    "    plt.ylabel(f'Total de {claim_type.replace(\"_\", \" \").title()}')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Criar gráfico total de sinistros\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=result_df, x='Codigo_Regiao', y='Total_Sinistros', palette='viridis')\n",
    "\n",
    "plt.title('Total Geral de Sinistros por Região')\n",
    "plt.xlabel('Código da Região')\n",
    "plt.ylabel('Total Geral de Sinistros')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9d95eaf-cd03-435d-9bb3-be27cf688190",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Top 5 Regiões com Maior Número Total de Sinistros\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Carregar os resultados da consulta SQL para um DataFrame\n",
    "result_df = spark.sql(\"SELECT * FROM analise_regiao\").toPandas()\n",
    "\n",
    "# Configurar o estilo do seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Calcular o total de sinistros para cada região\n",
    "result_df['Total_Sinistros'] = (\n",
    "    result_df['Total_Sinistros_Roubo_Furto'] +\n",
    "    result_df['Total_Sinistros_Colisao_Parcial'] +\n",
    "    result_df['Total_Sinistros_Colisao_Perdida_Total'] +\n",
    "    result_df['Total_Sinistros_Incendio'] +\n",
    "    result_df['Total_Sinistros_Outras']\n",
    ")\n",
    "\n",
    "# Filtrar e ordenar os dados para obter as top 5 regiões\n",
    "top_5_regioes = result_df.nlargest(5, 'Total_Sinistros')\n",
    "\n",
    "# Criar o gráfico\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=top_5_regioes, x='Codigo_Regiao', y='Total_Sinistros', palette='viridis')\n",
    "\n",
    "plt.title('Top 5 Regiões com Maior Número Total de Sinistros')\n",
    "plt.xlabel('Código da Região')\n",
    "plt.ylabel('Total de Sinistros')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36a06424-9673-4790-ad2b-9feb54140f37",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Tamanho da amostra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "35185ea5-bf9d-42fc-bab2-1d270fc43943",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Contar o número de registros que atendem aos critérios especificados\n",
    "sample_size_query = \"\"\"\n",
    "SELECT COUNT(*) AS Numero_Registros\n",
    "FROM arq_casco_comp_tratado v\n",
    "JOIN auto_cat_tratado a ON v.COD_TARIF = a.CODIGO\n",
    "WHERE a.CODIGO = 1  -- Apenas veículos de passeio\n",
    "  AND (2024 - v.ANO_MODELO) <= 10  -- Idade do veículo <= 10 anos\n",
    "  AND v.REGIAO NOT IN (11, 13)  -- Excluir regiões 11 e 13\n",
    "\"\"\"\n",
    "\n",
    "# Executar a consulta e carregar o resultado em um DataFrame\n",
    "sample_size_df = spark.sql(sample_size_query).toPandas()\n",
    "\n",
    "# Mostrar o número de registros\n",
    "print(f\"Número de registros na amostra: {sample_size_df['Numero_Registros'][0]}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Projeto_analise_dados_SUSEP",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
